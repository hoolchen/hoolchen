<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta name="keywords" content="hexo,个人博客,blog">
  <meta name="description" content="某某的个人博客">
  <meta http-equiv="x-dns-prefetch-control" content="on">
  <link rel="dns-prefetch" href="https://busuanzi.ibruce.info">
  
  <link rel="dns-prefetch" href="https://widget.daovoice.io">
  <link rel="dns-prefetch" href="https://widget-static-cdn.daovoice.io">
  <link rel="dns-prefetch" href="https://im.daovoice.io">
  
  
  <link rel="dns-prefetch" href="https://hm.baidu.com/">
  
  
  <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="https://api.github.com">
  <link rel="dns-prefetch" href="https://avatars3.githubusercontent.com">
  
  <link rel="stylesheet" type="text/css" href="/./style/main.d9e3dd.css">
	<link rel="shortcut icon" href="/favicon.ico" title="Favicon">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
	<title>随机梯度下降</title>
  
  <script>var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="https://hm.baidu.com/hm.js?awwssw1snsnsnn1ndndnndnd99j";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();
  </script>
  
  
    <script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/123456.js","daovoice");daovoice('init',{app_id: "123456"});daovoice('update');
  </script>
  
</head>
<body>
<canvas id="pattern-placeholder" height="230"></canvas>
<div class="navbar-header">
  <a class="blog-title" href="/">hoolchencodes</a>
  <a class="face-img" href="/">
    <img src="https://tva3.sinaimg.cn/crop.0.0.200.200.180/005Kc3C1jw8f2uep0hhkvj305k05k3yk.jpg">
  </a>
</div>
<main>
  <div class="article-title">
    
  
  <h1 class="title">
    随机梯度下降
  </h1>
  


    <ul class="article-info">
      <li>
        发布
        <time datetime="2019-03-27T07:58:34.000Z" itemprop="datePublished">2019-03-27</time>
      </li>
      <li>
        
    更新 <time datetime="2019-03-27T08:57:39.998Z" itemprop="dateUpdated">2019-03-27</time>

      </li>
      <li id="busuanzi_container_page_pv">
        阅读 <span id="busuanzi_value_page_pv"></span>
      </li>
    </ul>
  </div>
  <div class="container">
    <div class="article">
      <div class="content">
        
        <h2 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise 1"></a>Exercise 1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_data = np.loadtxt(<span class="string">"dataForTesting.txt"</span>)</span><br><span class="line">train_data = np.loadtxt(<span class="string">"dataForTraining.txt"</span>)</span><br></pre></td></tr></table></figure>
<p>损失函数如下，向量化计算公式<br>$$(1/2<em>m)</em>(X.T.dot(X.dot(gradient)-y))$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(x, y, gradient)</span>:</span></span><br><span class="line">    s = y.shape[<span class="number">0</span>] <span class="comment">#样本数量</span></span><br><span class="line">    C = x.dot(gradient) - y;</span><br><span class="line">    J2 = (C.T.dot(C)) / (<span class="number">2</span>*s)</span><br><span class="line">    <span class="keyword">return</span> J2</span><br></pre></td></tr></table></figure>
<p>随机梯度下降算法，即<br>Loop {<br>  for i=1 to m,{<br>    $$θ_j := θ<em>j + α(y^i - h</em>θ(x^i))x_j^i$$<br>  }<br>}</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(x, y, gradient, lr, loss_train, loss_test, p_x, py, m, iter_count)</span>:</span></span><br><span class="line">    s = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        r = random.randint(<span class="number">0</span>, s<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            gradient[j, <span class="number">0</span>] = gradient[j, <span class="number">0</span>] - lr * x[r, j] * (x[r, :].dot(gradient) - y[r, <span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> (int((i + <span class="number">1</span>) % iter_count) == <span class="number">0</span>):</span><br><span class="line">            loss_train[int(i / iter_count)] = computeCost(x, y, gradient)</span><br><span class="line">            loss_test[int(i / iter_count)] = computeCost(p_x, py, gradient)</span><br><span class="line">    <span class="keyword">return</span> gradient, loss_train, loss_test</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#特征增维</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">featureAddition</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.hstack([x, np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(data, mean, std, gradient)</span>:</span></span><br><span class="line">    data = (data - mean) / std;</span><br><span class="line">    data = np.hstack([data, np.ones((data.shape[<span class="number">0</span>], <span class="number">1</span>))])</span><br><span class="line">    price = data.dot(gradient)</span><br><span class="line">    <span class="keyword">return</span> price</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">3</span> <span class="comment">#特征数量</span></span><br><span class="line">gradient = np.zeros((n,<span class="number">1</span>)) <span class="comment">#初始化梯度</span></span><br><span class="line">lr = <span class="number">0.00015</span> <span class="comment">#学习率</span></span><br><span class="line">m = <span class="number">1500000</span> <span class="comment">#迭代次数</span></span><br><span class="line">iter_count = <span class="number">100000</span></span><br><span class="line">loss_test = np.zeros((int(m / iter_count), <span class="number">1</span>)) <span class="comment">#训练样本误差（100000步一次）</span></span><br><span class="line">loss_train = np.zeros((int(m / iter_count), <span class="number">1</span>))<span class="comment">#测试样本误差（100000步一次）</span></span><br><span class="line">X = train_data[:, (<span class="number">0</span>,<span class="number">1</span>)].reshape((<span class="number">-1</span>,<span class="number">2</span>))<span class="comment">#训练样本的X</span></span><br><span class="line">Y = train_data[:, <span class="number">2</span>].reshape((<span class="number">-1</span>,<span class="number">1</span>))    <span class="comment">#训练样本的Y</span></span><br><span class="line">px = test_data[:, (<span class="number">0</span>, <span class="number">1</span>)].reshape((<span class="number">-1</span>, <span class="number">2</span>))<span class="comment">#测试样本的X</span></span><br><span class="line">py = test_data[:, <span class="number">2</span>]. reshape((<span class="number">-1</span>, <span class="number">1</span>))    <span class="comment">#测试样本的Y</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = featureAddition(X) <span class="comment">#训练样本的特征增维</span></span><br><span class="line">x_test = featureAddition(px) <span class="comment">#测试样本的特征增维</span></span><br><span class="line">gradient, loss_train, loss_test = gradientDescent(x_train, Y, gradient, lr, loss_train, loss_test, x_test, py, m, iter_count)</span><br><span class="line">print(<span class="string">"梯度如下\n"</span>,gradient)</span><br></pre></td></tr></table></figure>
<pre><code>梯度如下
 [[nan]
 [nan]
 [nan]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练样本的误差</span></span><br><span class="line">plt.plot(loss_train)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>);</span><br><span class="line">plt.xlabel(<span class="string">"iter count/(100000)"</span>)</span><br><span class="line">plt.title(<span class="string">"loss_train function"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0.5, 1.0, &apos;loss_train function&apos;)
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_11_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试样本的误差</span></span><br><span class="line">plt.plot(loss_test)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>);</span><br><span class="line">plt.xlabel(<span class="string">"iter count/(100000)"</span>)</span><br><span class="line">plt.title(<span class="string">"loss_test function"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0.5, 1.0, &apos;loss_test function&apos;)
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_12_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测价格， 橙色为样本价格，蓝色为预测价格</span></span><br><span class="line">mean = <span class="number">0</span></span><br><span class="line">std = <span class="number">1</span></span><br><span class="line">price = predict(px, mean, std, gradient)</span><br><span class="line"><span class="comment"># print("预测价格如下\n", price)</span></span><br><span class="line"><span class="comment"># print("样本价格如下\n", py)</span></span><br><span class="line">plt.plot(price)</span><br><span class="line">plt.plot(py)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x236ec5c1898&gt;]
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_13_1.png" alt="png"></p>
<p>从上面图可以看出，由于学习率偏大，导致梯度变化幅度过大，loss增长太快，导致每次都越过最优解。</p>
<h2 id="Exercise-2"><a href="#Exercise-2" class="headerlink" title="Exercise 2"></a>Exercise 2</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gradient = np.zeros((n,<span class="number">1</span>)) <span class="comment">#初始化梯度</span></span><br><span class="line">lr = <span class="number">0.00002</span> <span class="comment">#学习率</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gradient, loss_train, loss_test = gradientDescent(x_train, Y, gradient, lr, loss_train, loss_test, x_test, py, m, iter_count)</span><br><span class="line">print(<span class="string">"梯度如下\n"</span>,gradient)</span><br></pre></td></tr></table></figure>
<pre><code>梯度如下
 [[  6.84244681]
 [-72.22161681]
 [ 70.50986559]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练样本的误差</span></span><br><span class="line">plt.plot(loss_train)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>);</span><br><span class="line">plt.xlabel(<span class="string">"iter count/(100000)"</span>)</span><br><span class="line">plt.title(<span class="string">"loss_train function"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0.5, 1.0, &apos;loss_train function&apos;)
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_18_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试样本的误差</span></span><br><span class="line">plt.plot(loss_test)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>);</span><br><span class="line">plt.xlabel(<span class="string">"iter count/(100000)"</span>)</span><br><span class="line">plt.title(<span class="string">"loss_test function"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0.5, 1.0, &apos;loss_test function&apos;)
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_19_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测价格，橙色为样本价格，蓝色为预测价格</span></span><br><span class="line">mean = <span class="number">0</span></span><br><span class="line">std = <span class="number">1</span></span><br><span class="line">price = predict(px, mean, std, gradient)</span><br><span class="line"><span class="comment"># print("预测价格如下\n", price)</span></span><br><span class="line"><span class="comment"># print("样本价格如下\n", py)</span></span><br><span class="line">plt.plot(price)</span><br><span class="line">plt.plot(py)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x236ec6f02b0&gt;]
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_20_1.png" alt="png"></p>
<p>当学习率从0.00015降到0.00002，即学习率减小，loss增长幅度变小，容易找到最优解。</p>
<h2 id="Exercise-3"><a href="#Exercise-3" class="headerlink" title="Exercise 3"></a>Exercise 3</h2><p>在这里，我引用特征缩放，标准化数据样本特征的范围<br>$$x_n = \frac{x_n - mean}{sigma}$$<br>其中mean为样本特征的平均值，sigma是样本特征的标准差，可以将特征的尺度缩放到-1到1之间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#特征缩放</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">featureNormalize</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    x_norm = x</span><br><span class="line">    mean = np.zeros((<span class="number">1</span>, n<span class="number">-1</span>))</span><br><span class="line">    std = np.zeros((<span class="number">1</span>, n<span class="number">-1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        mean[<span class="number">0</span>, i] = np.mean(x[:, i])</span><br><span class="line">        std[<span class="number">0</span>, i] = np.std(x[:, i])</span><br><span class="line">    x_norm = (x - mean) / std</span><br><span class="line">    <span class="keyword">return</span> x_norm, mean, std</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gradient = np.zeros((n,<span class="number">1</span>)) <span class="comment">#初始化梯度</span></span><br><span class="line">lr = <span class="number">0.0002</span> <span class="comment">#学习率</span></span><br><span class="line">m = <span class="number">30000</span> <span class="comment">#迭代次数</span></span><br><span class="line">iter_count = <span class="number">1000</span></span><br><span class="line">loss_test = np.zeros((int(m / iter_count), <span class="number">1</span>)) <span class="comment">#训练样本误差（10000步一次）</span></span><br><span class="line">loss_train = np.zeros((int(m / iter_count), <span class="number">1</span>))<span class="comment">#测试样本误差（10000步一次）</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train, mean, std = featureNormalize(X, n) <span class="comment">#训练样本的特征缩放</span></span><br><span class="line">x_train = featureAddition(x_train)</span><br><span class="line">x_test, mean_test, std_test = featureNormalize(px, n) <span class="comment">#测试样本的特征缩放</span></span><br><span class="line">x_test = featureAddition(x_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gradient, loss_train, loss_test = gradientDescent(x_train, Y, gradient, lr, loss_train, loss_test, x_test, py, m, iter_count)</span><br><span class="line">print(<span class="string">"梯度如下\n"</span>,gradient)</span><br></pre></td></tr></table></figure>
<pre><code>梯度如下
 [[ 159.24297428]
 [-200.38232214]
 [ 400.45342326]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练样本的误差</span></span><br><span class="line">plt.plot(loss_train)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>);</span><br><span class="line">plt.xlabel(<span class="string">"iter count/(1000)"</span>)</span><br><span class="line">plt.title(<span class="string">"loss_train function"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0.5, 1.0, &apos;loss_train function&apos;)
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_28_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试样本的误差</span></span><br><span class="line">plt.plot(loss_test)</span><br><span class="line">plt.ylabel(<span class="string">"loss"</span>);</span><br><span class="line">plt.xlabel(<span class="string">"iter count/(1000)"</span>)</span><br><span class="line">plt.title(<span class="string">"loss_test function"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0.5, 1.0, &apos;loss_test function&apos;)
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_29_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测价格，橙色为样本价格，蓝色为预测价格</span></span><br><span class="line">price = predict(px, mean, std, gradient)</span><br><span class="line"><span class="comment"># print("预测价格如下\n", price)</span></span><br><span class="line"><span class="comment"># print("样本价格如下\n", py)</span></span><br><span class="line">plt.plot(price)</span><br><span class="line">plt.plot(py)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x236ec910b00&gt;]
</code></pre><p><img src="/2019/03/27/数据挖掘作业一/output_30_1.png" alt="png"></p>
<p>经过特征缩放以及合适的学习率，在较短的迭代次数中，已得到最优解</p>

      </div>
        <div class="support-author">
          <p>感谢您的阅读。 🙏
          <a href="https://888.com/index.html" target="_blank">关于转载请看这里</a>
            <!--<a class="btn-pay"  href="#pay-modal">¥ 打赏支持</a>-->
          </p>
        </div>
        <!--
            <div class="like ">
              <div class="like-button">
                <a id="like-note" href="">
                  <i class="icon-heart"></i>喜欢
                </a>
              </div>
              <span id="likes-count">256</span>
            </div>
        -->
        <div class="otherLink">
          <div class="previous">
          </div>
          <div class="next">
          </div>
        </div>
        <div class="comments" id="comments">
          
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script type="text/javascript">
  const gitalk = new Gitalk({
    clientID: 'ca381225e22ce3127684',
    clientSecret: '7dd4984948043f76543b7d87985869470b9a31de',
    repo: 'hoolchen_comments',
    owner: 'hoolchen',
    admin: ['hoolchen'],
    id: decodeURI(location.pathname),      // Ensure uniqueness and length less than 50
    distractionFreeMode: true
  })

  gitalk.render('comments');
</script>


        </div>
      </div>
    </div>
   
</main>
<div class="footer">
  <div class="info">
    <p>
    <a href="https://hexo.io"> Hexo </a> 强力驱动 |
      <a href="https://github.com/Youthink/hexo-themes-yearn"> Yearn </a>
      主题
    </p>
    <p>&copy;2013-2018 某某的博客 京ICP备xxxxxx号</p>
  </div>
</div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script>//console
  var consoleConfig = '\n欢迎访问 https://hufangyun.com ，围观小猿大圣的博客(づ｡◕‿‿◕｡)づ！\n,\n本博客使用 %cHexo%c 搭建，博客主题为小猿大圣开发的 %chexo-themes-yearn%c ~~~ 🎉🎉🎉 \n\n源码 https://github.com/Youthink/hexo-themes-yearn \n\n如果喜欢可以 star 支持一下 ❤️~\n,\n扫描下面的二维码，在手机上查看博客！\n,https://static.hufangyun.com/blog-url-qrcode-180-180.png,\n 想知道这个效果如何实现的？博客内搜索 console 彩蛋 🚀 ！\n'.split(',');
  var canConsole = true;
  var consoleInfo = (function(consoleConfig) {
  if (!canConsole || !consoleConfig || consoleConfig.length < 1) {
    return;
  }
  var consoleColor = '#6190e8';
  var _console;
  var backgroundTextStyle = 'padding: 1px 5px;color: #fff;background: ' + consoleColor + ';'
  var textStyle = 'color: ' + consoleColor + ';';

  consoleConfig.map(o => {
    var num = (o.match(/%c/g) || []).length;
    if(/^http(s)?:\/\//.test(o)) {
      console.log('%c     ', 'background: url(' + o + ') no-repeat left center;font-size: 180px;');
      return;
    }
    if (num > 0) {
      var logArguments = [];
      for (var i = 0; i < num; i++) {
        if (i % 2 === 0) {
          logArguments.push(backgroundTextStyle);
        } else {
          logArguments.push(textStyle);
        }
      }
      (_console = console).log.apply(_console, ['%c' + o, textStyle].concat(logArguments));
      return;
    }
    console.log('%c' + o, textStyle);
  });
}(consoleConfig));</script><script type="text/javascript" src="/./js/main.d9e3dd.js"></script>

</body>
</html>
